{
    "learning_rate": 5e-5,
    "num_epochs": 5,
    "batch_size": 8,
    "max_seq_len": 1024,
    "vocab_size": 50257,
    "embedding_dim": 768,
    "num_heads": 12,
    "num_layers": 12,
    "dropout": 0.1,
    "num_warmup_steps": 500,
    "data": {
      "processed": {
        "tokenized_data": "Path to Tokenised data"
      }
    },
    "results": {
      "logs": "Path to logs"
    },
    "models": {
      "checkpoints": "Path to save model checkpoints"
    }
  }
  
